{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitToEmoji-GIT_modified.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijitnayak8/abhijitnayak8/blob/main/FruitToEmoji_GIT_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f92-4Hjy7kA8"
      },
      "source": [
        "<a href=\"https://www.arduino.cc/\"><img src=\"https://raw.githubusercontent.com/sandeepmistry/aimldevfest-workshop-2019/master/images/Arduino_logo_R_highquality.png\" width=200/></a>\n",
        "# Tiny ML on Arduino\n",
        "## Classify objects by color tutorial\n",
        "\n",
        " \n",
        "https://github.com/arduino/ArduinoTensorFlowLiteTutorials/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment \n",
        "\n",
        "The next cell sets up the dependencies in required for the notebook, run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gs-PL4xDkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8fece22-8647-4677-b132-fdd8282a0b66"
      },
      "source": [
        "# Setup environment\n",
        "# !apt-get -qq install xxd\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.88.142)] [1\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.88.142)] [C\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [80.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [932 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,257 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,829 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,067 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,628 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [937 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,479 kB]\n",
            "Get:24 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.2 kB]\n",
            "Fetched 13.5 MB in 5s (2,863 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xxd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 68 not upgraded.\n",
            "Need to get 49.9 kB of archives.\n",
            "After this operation, 201 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xxd amd64 2:8.0.1453-1ubuntu1.8 [49.9 kB]\n",
            "Fetched 49.9 kB in 0s (158 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xxd.\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.8_amd64.deb ...\n",
            "Unpacking xxd (2:8.0.1453-1ubuntu1.8) ...\n",
            "Setting up xxd (2:8.0.1453-1ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "# Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the Files tab\n",
        "1. Drag `csv` files from your computer to the tab to upload them into colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj-C4CPnppJY",
        "outputId": "11a91f20-c29a-4596-b436-affeb13b511a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxUeYPNQbOg"
      },
      "source": [
        "# Train Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGChd1FAk5_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "44934820-8d3b-4033-a190-76c37f2eafc5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import fileinput\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "CLASSES = [];\n",
        "\n",
        "for file in os.listdir(\"/content/\"):\n",
        "    if file.endswith(\".csv\"):\n",
        "        CLASSES.append(os.path.splitext(file)[0])\n",
        "\n",
        "CLASSES.sort()\n",
        "\n",
        "SAMPLES_WINDOW_LEN = 1\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for class_index in range(NUM_CLASSES):\n",
        "  objectClass = CLASSES[class_index]\n",
        "  df = pd.read_csv(\"/content/\" + objectClass + \".csv\")\n",
        "  columns = list(df)\n",
        "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "   \n",
        "  # calculate the number of objectClass recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_WINDOW_LEN)\n",
        "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
        "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
        "\n",
        "  # graphing\n",
        "  plt.rcParams[\"figure.figsize\"] = (10,1)\n",
        "  pixels = np.array([df['Red'],df['Green'],df['Blue']],float)\n",
        "  pixels = np.transpose(pixels)\n",
        "  for i in range(num_recordings):\n",
        "    plt.axvline(x=i, linewidth=8, color=tuple(pixels[i]/np.max(pixels[i], axis=0)))\n",
        "  plt.show()\n",
        "  \n",
        "  #tensors\n",
        "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    row = []\n",
        "    for c in columns:\n",
        "      row.append(df[c][i])\n",
        "    tensor += row\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")\n",
        "\n",
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version = 2.8.0\n",
            "\n",
            "\u001b[32;4mnano\u001b[0m class will be output \u001b[32m0\u001b[0m of the classifier\n",
            "173 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;4mpeipa\u001b[0m class will be output \u001b[32m1\u001b[0m of the classifier\n",
            "256 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;4mrpi\u001b[0m class will be output \u001b[32m2\u001b[0m of the classifier\n",
            "210 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set parsing and preparation complete.\n",
            "Data set randomization and splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8qlSAX1b6Yv"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGNFa-lX24Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fba220-5980-44b1-8ba1-a16155dae606"
      },
      "source": [
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=400, batch_size=4, validation_data=(inputs_validate, outputs_validate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "96/96 [==============================] - 1s 3ms/step - loss: 0.2117 - mae: 0.4314 - val_loss: 0.2076 - val_mae: 0.4271\n",
            "Epoch 2/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2071 - mae: 0.4260 - val_loss: 0.2022 - val_mae: 0.4207\n",
            "Epoch 3/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2005 - mae: 0.4187 - val_loss: 0.1942 - val_mae: 0.4113\n",
            "Epoch 4/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1905 - mae: 0.4071 - val_loss: 0.1824 - val_mae: 0.3977\n",
            "Epoch 5/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1769 - mae: 0.3900 - val_loss: 0.1690 - val_mae: 0.3796\n",
            "Epoch 6/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.3718 - val_loss: 0.1569 - val_mae: 0.3607\n",
            "Epoch 7/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3530 - val_loss: 0.1467 - val_mae: 0.3426\n",
            "Epoch 8/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1424 - mae: 0.3342 - val_loss: 0.1376 - val_mae: 0.3237\n",
            "Epoch 9/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.3159 - val_loss: 0.1310 - val_mae: 0.3083\n",
            "Epoch 10/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1276 - mae: 0.3019 - val_loss: 0.1257 - val_mae: 0.2953\n",
            "Epoch 11/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1224 - mae: 0.2894 - val_loss: 0.1219 - val_mae: 0.2857\n",
            "Epoch 12/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1179 - mae: 0.2796 - val_loss: 0.1188 - val_mae: 0.2784\n",
            "Epoch 13/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1137 - mae: 0.2717 - val_loss: 0.1139 - val_mae: 0.2680\n",
            "Epoch 14/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1092 - mae: 0.2627 - val_loss: 0.1111 - val_mae: 0.2628\n",
            "Epoch 15/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1046 - mae: 0.2547 - val_loss: 0.1062 - val_mae: 0.2540\n",
            "Epoch 16/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.1001 - mae: 0.2470 - val_loss: 0.1022 - val_mae: 0.2473\n",
            "Epoch 17/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0952 - mae: 0.2391 - val_loss: 0.0973 - val_mae: 0.2389\n",
            "Epoch 18/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0906 - mae: 0.2314 - val_loss: 0.0932 - val_mae: 0.2320\n",
            "Epoch 19/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0856 - mae: 0.2233 - val_loss: 0.0893 - val_mae: 0.2260\n",
            "Epoch 20/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0811 - mae: 0.2156 - val_loss: 0.0843 - val_mae: 0.2174\n",
            "Epoch 21/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0767 - mae: 0.2077 - val_loss: 0.0804 - val_mae: 0.2110\n",
            "Epoch 22/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0724 - mae: 0.2001 - val_loss: 0.0764 - val_mae: 0.2040\n",
            "Epoch 23/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0683 - mae: 0.1931 - val_loss: 0.0721 - val_mae: 0.1960\n",
            "Epoch 24/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0640 - mae: 0.1847 - val_loss: 0.0680 - val_mae: 0.1885\n",
            "Epoch 25/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0605 - mae: 0.1777 - val_loss: 0.0644 - val_mae: 0.1817\n",
            "Epoch 26/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0563 - mae: 0.1693 - val_loss: 0.0620 - val_mae: 0.1757\n",
            "Epoch 27/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0535 - mae: 0.1633 - val_loss: 0.0577 - val_mae: 0.1674\n",
            "Epoch 28/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0505 - mae: 0.1561 - val_loss: 0.0550 - val_mae: 0.1615\n",
            "Epoch 29/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0475 - mae: 0.1494 - val_loss: 0.0528 - val_mae: 0.1553\n",
            "Epoch 30/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0454 - mae: 0.1440 - val_loss: 0.0494 - val_mae: 0.1488\n",
            "Epoch 31/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0432 - mae: 0.1390 - val_loss: 0.0474 - val_mae: 0.1434\n",
            "Epoch 32/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0414 - mae: 0.1337 - val_loss: 0.0456 - val_mae: 0.1385\n",
            "Epoch 33/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0396 - mae: 0.1290 - val_loss: 0.0433 - val_mae: 0.1334\n",
            "Epoch 34/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0379 - mae: 0.1245 - val_loss: 0.0426 - val_mae: 0.1298\n",
            "Epoch 35/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0360 - mae: 0.1197 - val_loss: 0.0399 - val_mae: 0.1239\n",
            "Epoch 36/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0345 - mae: 0.1156 - val_loss: 0.0401 - val_mae: 0.1214\n",
            "Epoch 37/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0333 - mae: 0.1113 - val_loss: 0.0370 - val_mae: 0.1159\n",
            "Epoch 38/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0320 - mae: 0.1082 - val_loss: 0.0358 - val_mae: 0.1122\n",
            "Epoch 39/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0307 - mae: 0.1037 - val_loss: 0.0348 - val_mae: 0.1091\n",
            "Epoch 40/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0292 - mae: 0.1010 - val_loss: 0.0342 - val_mae: 0.1053\n",
            "Epoch 41/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0286 - mae: 0.0978 - val_loss: 0.0326 - val_mae: 0.1016\n",
            "Epoch 42/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0275 - mae: 0.0950 - val_loss: 0.0316 - val_mae: 0.0991\n",
            "Epoch 43/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0268 - mae: 0.0920 - val_loss: 0.0311 - val_mae: 0.0975\n",
            "Epoch 44/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0259 - mae: 0.0897 - val_loss: 0.0303 - val_mae: 0.0948\n",
            "Epoch 45/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0252 - mae: 0.0879 - val_loss: 0.0301 - val_mae: 0.0937\n",
            "Epoch 46/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0246 - mae: 0.0858 - val_loss: 0.0287 - val_mae: 0.0893\n",
            "Epoch 47/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0237 - mae: 0.0835 - val_loss: 0.0281 - val_mae: 0.0868\n",
            "Epoch 48/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0229 - mae: 0.0796 - val_loss: 0.0285 - val_mae: 0.0880\n",
            "Epoch 49/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.0793 - val_loss: 0.0269 - val_mae: 0.0828\n",
            "Epoch 50/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0216 - mae: 0.0768 - val_loss: 0.0271 - val_mae: 0.0832\n",
            "Epoch 51/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0211 - mae: 0.0757 - val_loss: 0.0260 - val_mae: 0.0779\n",
            "Epoch 52/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0205 - mae: 0.0735 - val_loss: 0.0254 - val_mae: 0.0761\n",
            "Epoch 53/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0201 - mae: 0.0711 - val_loss: 0.0251 - val_mae: 0.0763\n",
            "Epoch 54/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0197 - mae: 0.0709 - val_loss: 0.0246 - val_mae: 0.0744\n",
            "Epoch 55/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0190 - mae: 0.0690 - val_loss: 0.0243 - val_mae: 0.0715\n",
            "Epoch 56/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0186 - mae: 0.0675 - val_loss: 0.0238 - val_mae: 0.0703\n",
            "Epoch 57/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0183 - mae: 0.0662 - val_loss: 0.0235 - val_mae: 0.0695\n",
            "Epoch 58/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0180 - mae: 0.0646 - val_loss: 0.0233 - val_mae: 0.0693\n",
            "Epoch 59/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0174 - mae: 0.0636 - val_loss: 0.0228 - val_mae: 0.0662\n",
            "Epoch 60/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0171 - mae: 0.0620 - val_loss: 0.0228 - val_mae: 0.0641\n",
            "Epoch 61/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0170 - mae: 0.0607 - val_loss: 0.0224 - val_mae: 0.0652\n",
            "Epoch 62/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0594 - val_loss: 0.0238 - val_mae: 0.0699\n",
            "Epoch 63/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0164 - mae: 0.0600 - val_loss: 0.0219 - val_mae: 0.0614\n",
            "Epoch 64/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0157 - mae: 0.0581 - val_loss: 0.0218 - val_mae: 0.0602\n",
            "Epoch 65/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0561 - val_loss: 0.0215 - val_mae: 0.0614\n",
            "Epoch 66/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0154 - mae: 0.0563 - val_loss: 0.0214 - val_mae: 0.0611\n",
            "Epoch 67/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0149 - mae: 0.0547 - val_loss: 0.0211 - val_mae: 0.0594\n",
            "Epoch 68/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0544 - val_loss: 0.0208 - val_mae: 0.0579\n",
            "Epoch 69/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0145 - mae: 0.0526 - val_loss: 0.0206 - val_mae: 0.0573\n",
            "Epoch 70/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0143 - mae: 0.0524 - val_loss: 0.0209 - val_mae: 0.0586\n",
            "Epoch 71/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0139 - mae: 0.0517 - val_loss: 0.0220 - val_mae: 0.0625\n",
            "Epoch 72/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0138 - mae: 0.0516 - val_loss: 0.0201 - val_mae: 0.0546\n",
            "Epoch 73/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0135 - mae: 0.0500 - val_loss: 0.0204 - val_mae: 0.0567\n",
            "Epoch 74/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0135 - mae: 0.0494 - val_loss: 0.0198 - val_mae: 0.0536\n",
            "Epoch 75/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0128 - mae: 0.0490 - val_loss: 0.0196 - val_mae: 0.0519\n",
            "Epoch 76/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0479 - val_loss: 0.0196 - val_mae: 0.0515\n",
            "Epoch 77/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0475 - val_loss: 0.0195 - val_mae: 0.0492\n",
            "Epoch 78/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0461 - val_loss: 0.0193 - val_mae: 0.0486\n",
            "Epoch 79/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0446 - val_loss: 0.0192 - val_mae: 0.0501\n",
            "Epoch 80/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0452 - val_loss: 0.0190 - val_mae: 0.0481\n",
            "Epoch 81/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0441 - val_loss: 0.0191 - val_mae: 0.0497\n",
            "Epoch 82/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0444 - val_loss: 0.0189 - val_mae: 0.0466\n",
            "Epoch 83/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0120 - mae: 0.0429 - val_loss: 0.0188 - val_mae: 0.0476\n",
            "Epoch 84/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0414 - val_loss: 0.0192 - val_mae: 0.0499\n",
            "Epoch 85/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0418 - val_loss: 0.0185 - val_mae: 0.0460\n",
            "Epoch 86/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0414 - val_loss: 0.0186 - val_mae: 0.0465\n",
            "Epoch 87/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0110 - mae: 0.0409 - val_loss: 0.0186 - val_mae: 0.0431\n",
            "Epoch 88/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0391 - val_loss: 0.0186 - val_mae: 0.0464\n",
            "Epoch 89/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0404 - val_loss: 0.0182 - val_mae: 0.0435\n",
            "Epoch 90/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0394 - val_loss: 0.0194 - val_mae: 0.0423\n",
            "Epoch 91/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0389 - val_loss: 0.0182 - val_mae: 0.0442\n",
            "Epoch 92/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0383 - val_loss: 0.0183 - val_mae: 0.0451\n",
            "Epoch 93/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0103 - mae: 0.0392 - val_loss: 0.0184 - val_mae: 0.0453\n",
            "Epoch 94/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0382 - val_loss: 0.0182 - val_mae: 0.0409\n",
            "Epoch 95/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0370 - val_loss: 0.0179 - val_mae: 0.0418\n",
            "Epoch 96/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0369 - val_loss: 0.0178 - val_mae: 0.0407\n",
            "Epoch 97/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0104 - mae: 0.0365 - val_loss: 0.0178 - val_mae: 0.0402\n",
            "Epoch 98/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0355 - val_loss: 0.0178 - val_mae: 0.0396\n",
            "Epoch 99/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0366 - val_loss: 0.0178 - val_mae: 0.0410\n",
            "Epoch 100/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0355 - val_loss: 0.0178 - val_mae: 0.0418\n",
            "Epoch 101/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0351 - val_loss: 0.0178 - val_mae: 0.0387\n",
            "Epoch 102/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0348 - val_loss: 0.0175 - val_mae: 0.0393\n",
            "Epoch 103/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0349 - val_loss: 0.0176 - val_mae: 0.0383\n",
            "Epoch 104/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0344 - val_loss: 0.0175 - val_mae: 0.0387\n",
            "Epoch 105/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0095 - mae: 0.0338 - val_loss: 0.0178 - val_mae: 0.0412\n",
            "Epoch 106/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0335 - val_loss: 0.0175 - val_mae: 0.0394\n",
            "Epoch 107/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0096 - mae: 0.0332 - val_loss: 0.0175 - val_mae: 0.0372\n",
            "Epoch 108/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0327 - val_loss: 0.0174 - val_mae: 0.0371\n",
            "Epoch 109/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0096 - mae: 0.0322 - val_loss: 0.0176 - val_mae: 0.0367\n",
            "Epoch 110/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0327 - val_loss: 0.0173 - val_mae: 0.0368\n",
            "Epoch 111/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0095 - mae: 0.0320 - val_loss: 0.0173 - val_mae: 0.0374\n",
            "Epoch 112/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0323 - val_loss: 0.0175 - val_mae: 0.0356\n",
            "Epoch 113/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0091 - mae: 0.0308 - val_loss: 0.0180 - val_mae: 0.0416\n",
            "Epoch 114/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0094 - mae: 0.0318 - val_loss: 0.0173 - val_mae: 0.0375\n",
            "Epoch 115/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0091 - mae: 0.0310 - val_loss: 0.0173 - val_mae: 0.0349\n",
            "Epoch 116/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0308 - val_loss: 0.0172 - val_mae: 0.0349\n",
            "Epoch 117/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0306 - val_loss: 0.0177 - val_mae: 0.0349\n",
            "Epoch 118/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0304 - val_loss: 0.0171 - val_mae: 0.0349\n",
            "Epoch 119/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0298 - val_loss: 0.0178 - val_mae: 0.0345\n",
            "Epoch 120/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0087 - mae: 0.0286 - val_loss: 0.0176 - val_mae: 0.0391\n",
            "Epoch 121/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0300 - val_loss: 0.0171 - val_mae: 0.0339\n",
            "Epoch 122/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0091 - mae: 0.0290 - val_loss: 0.0170 - val_mae: 0.0349\n",
            "Epoch 123/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0090 - mae: 0.0294 - val_loss: 0.0170 - val_mae: 0.0343\n",
            "Epoch 124/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0292 - val_loss: 0.0171 - val_mae: 0.0354\n",
            "Epoch 125/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0286 - val_loss: 0.0172 - val_mae: 0.0360\n",
            "Epoch 126/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0291 - val_loss: 0.0174 - val_mae: 0.0328\n",
            "Epoch 127/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0279 - val_loss: 0.0171 - val_mae: 0.0353\n",
            "Epoch 128/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0284 - val_loss: 0.0170 - val_mae: 0.0343\n",
            "Epoch 129/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0085 - mae: 0.0286 - val_loss: 0.0170 - val_mae: 0.0326\n",
            "Epoch 130/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0279 - val_loss: 0.0170 - val_mae: 0.0323\n",
            "Epoch 131/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0087 - mae: 0.0274 - val_loss: 0.0171 - val_mae: 0.0322\n",
            "Epoch 132/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0273 - val_loss: 0.0169 - val_mae: 0.0332\n",
            "Epoch 133/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0087 - mae: 0.0271 - val_loss: 0.0169 - val_mae: 0.0335\n",
            "Epoch 134/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0169 - val_mae: 0.0319\n",
            "Epoch 135/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0085 - mae: 0.0272 - val_loss: 0.0170 - val_mae: 0.0317\n",
            "Epoch 136/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0272 - val_loss: 0.0168 - val_mae: 0.0327\n",
            "Epoch 137/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0168 - val_mae: 0.0324\n",
            "Epoch 138/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0263 - val_loss: 0.0168 - val_mae: 0.0322\n",
            "Epoch 139/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0257 - val_loss: 0.0168 - val_mae: 0.0330\n",
            "Epoch 140/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0267 - val_loss: 0.0169 - val_mae: 0.0337\n",
            "Epoch 141/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0274 - val_loss: 0.0169 - val_mae: 0.0338\n",
            "Epoch 142/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0085 - mae: 0.0270 - val_loss: 0.0168 - val_mae: 0.0314\n",
            "Epoch 143/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0085 - mae: 0.0261 - val_loss: 0.0169 - val_mae: 0.0335\n",
            "Epoch 144/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0085 - mae: 0.0263 - val_loss: 0.0168 - val_mae: 0.0322\n",
            "Epoch 145/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0257 - val_loss: 0.0169 - val_mae: 0.0337\n",
            "Epoch 146/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0268 - val_loss: 0.0167 - val_mae: 0.0319\n",
            "Epoch 147/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0262 - val_loss: 0.0167 - val_mae: 0.0311\n",
            "Epoch 148/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0083 - mae: 0.0254 - val_loss: 0.0172 - val_mae: 0.0353\n",
            "Epoch 149/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0257 - val_loss: 0.0167 - val_mae: 0.0319\n",
            "Epoch 150/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0083 - mae: 0.0254 - val_loss: 0.0167 - val_mae: 0.0318\n",
            "Epoch 151/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0255 - val_loss: 0.0167 - val_mae: 0.0315\n",
            "Epoch 152/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0250 - val_loss: 0.0167 - val_mae: 0.0313\n",
            "Epoch 153/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0083 - mae: 0.0254 - val_loss: 0.0166 - val_mae: 0.0306\n",
            "Epoch 154/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0248 - val_loss: 0.0170 - val_mae: 0.0298\n",
            "Epoch 155/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0242 - val_loss: 0.0167 - val_mae: 0.0300\n",
            "Epoch 156/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0243 - val_loss: 0.0169 - val_mae: 0.0296\n",
            "Epoch 157/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0243 - val_loss: 0.0167 - val_mae: 0.0297\n",
            "Epoch 158/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0168 - val_mae: 0.0328\n",
            "Epoch 159/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0253 - val_loss: 0.0167 - val_mae: 0.0294\n",
            "Epoch 160/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0237 - val_loss: 0.0166 - val_mae: 0.0306\n",
            "Epoch 161/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0245 - val_loss: 0.0167 - val_mae: 0.0318\n",
            "Epoch 162/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0240 - val_loss: 0.0167 - val_mae: 0.0314\n",
            "Epoch 163/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0243 - val_loss: 0.0166 - val_mae: 0.0300\n",
            "Epoch 164/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0240 - val_loss: 0.0166 - val_mae: 0.0309\n",
            "Epoch 165/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0249 - val_loss: 0.0167 - val_mae: 0.0318\n",
            "Epoch 166/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0244 - val_loss: 0.0166 - val_mae: 0.0294\n",
            "Epoch 167/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0246 - val_loss: 0.0165 - val_mae: 0.0292\n",
            "Epoch 168/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0236 - val_loss: 0.0166 - val_mae: 0.0291\n",
            "Epoch 169/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0236 - val_loss: 0.0166 - val_mae: 0.0313\n",
            "Epoch 170/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0240 - val_loss: 0.0167 - val_mae: 0.0314\n",
            "Epoch 171/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0233 - val_loss: 0.0166 - val_mae: 0.0309\n",
            "Epoch 172/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0235 - val_loss: 0.0165 - val_mae: 0.0292\n",
            "Epoch 173/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0240 - val_loss: 0.0165 - val_mae: 0.0306\n",
            "Epoch 174/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0232 - val_loss: 0.0166 - val_mae: 0.0305\n",
            "Epoch 175/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0241 - val_loss: 0.0165 - val_mae: 0.0287\n",
            "Epoch 176/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0233 - val_loss: 0.0164 - val_mae: 0.0290\n",
            "Epoch 177/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0233 - val_loss: 0.0165 - val_mae: 0.0287\n",
            "Epoch 178/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0233 - val_loss: 0.0164 - val_mae: 0.0292\n",
            "Epoch 179/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0229 - val_loss: 0.0164 - val_mae: 0.0294\n",
            "Epoch 180/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0230 - val_loss: 0.0166 - val_mae: 0.0309\n",
            "Epoch 181/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0235 - val_loss: 0.0166 - val_mae: 0.0310\n",
            "Epoch 182/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0226 - val_loss: 0.0168 - val_mae: 0.0315\n",
            "Epoch 183/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0227 - val_loss: 0.0164 - val_mae: 0.0281\n",
            "Epoch 184/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0229 - val_loss: 0.0164 - val_mae: 0.0283\n",
            "Epoch 185/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0230 - val_loss: 0.0164 - val_mae: 0.0294\n",
            "Epoch 186/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0229 - val_loss: 0.0164 - val_mae: 0.0293\n",
            "Epoch 187/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0227 - val_loss: 0.0165 - val_mae: 0.0279\n",
            "Epoch 188/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0227 - val_loss: 0.0165 - val_mae: 0.0300\n",
            "Epoch 189/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0234 - val_loss: 0.0166 - val_mae: 0.0305\n",
            "Epoch 190/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0231 - val_loss: 0.0164 - val_mae: 0.0277\n",
            "Epoch 191/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0167 - val_mae: 0.0309\n",
            "Epoch 192/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0164 - val_mae: 0.0290\n",
            "Epoch 193/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0222 - val_loss: 0.0165 - val_mae: 0.0276\n",
            "Epoch 194/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0226 - val_loss: 0.0164 - val_mae: 0.0276\n",
            "Epoch 195/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0216 - val_loss: 0.0164 - val_mae: 0.0291\n",
            "Epoch 196/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0216 - val_loss: 0.0164 - val_mae: 0.0283\n",
            "Epoch 197/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0222 - val_loss: 0.0164 - val_mae: 0.0286\n",
            "Epoch 198/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0212 - val_loss: 0.0171 - val_mae: 0.0332\n",
            "Epoch 199/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0226 - val_loss: 0.0163 - val_mae: 0.0277\n",
            "Epoch 200/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0218 - val_loss: 0.0165 - val_mae: 0.0273\n",
            "Epoch 201/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0213 - val_loss: 0.0171 - val_mae: 0.0278\n",
            "Epoch 202/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0215 - val_loss: 0.0166 - val_mae: 0.0269\n",
            "Epoch 203/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0212 - val_loss: 0.0165 - val_mae: 0.0271\n",
            "Epoch 204/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0220 - val_loss: 0.0163 - val_mae: 0.0280\n",
            "Epoch 205/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0220 - val_loss: 0.0164 - val_mae: 0.0284\n",
            "Epoch 206/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0163 - val_mae: 0.0277\n",
            "Epoch 207/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0217 - val_loss: 0.0164 - val_mae: 0.0285\n",
            "Epoch 208/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0222 - val_loss: 0.0165 - val_mae: 0.0270\n",
            "Epoch 209/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0215 - val_loss: 0.0163 - val_mae: 0.0276\n",
            "Epoch 210/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0212 - val_loss: 0.0164 - val_mae: 0.0283\n",
            "Epoch 211/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0213 - val_loss: 0.0163 - val_mae: 0.0270\n",
            "Epoch 212/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0218 - val_loss: 0.0163 - val_mae: 0.0271\n",
            "Epoch 213/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0212 - val_loss: 0.0165 - val_mae: 0.0292\n",
            "Epoch 214/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0214 - val_loss: 0.0163 - val_mae: 0.0280\n",
            "Epoch 215/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0220 - val_loss: 0.0163 - val_mae: 0.0270\n",
            "Epoch 216/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0077 - mae: 0.0212 - val_loss: 0.0163 - val_mae: 0.0276\n",
            "Epoch 217/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0209 - val_loss: 0.0164 - val_mae: 0.0284\n",
            "Epoch 218/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0215 - val_loss: 0.0163 - val_mae: 0.0268\n",
            "Epoch 219/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0209 - val_loss: 0.0164 - val_mae: 0.0264\n",
            "Epoch 220/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0208 - val_loss: 0.0163 - val_mae: 0.0263\n",
            "Epoch 221/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0204 - val_loss: 0.0163 - val_mae: 0.0272\n",
            "Epoch 222/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0217 - val_loss: 0.0164 - val_mae: 0.0266\n",
            "Epoch 223/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0212 - val_loss: 0.0163 - val_mae: 0.0269\n",
            "Epoch 224/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0210 - val_loss: 0.0163 - val_mae: 0.0278\n",
            "Epoch 225/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0214 - val_loss: 0.0166 - val_mae: 0.0299\n",
            "Epoch 226/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0217 - val_loss: 0.0163 - val_mae: 0.0278\n",
            "Epoch 227/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0211 - val_loss: 0.0164 - val_mae: 0.0267\n",
            "Epoch 228/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0073 - mae: 0.0209 - val_loss: 0.0163 - val_mae: 0.0265\n",
            "Epoch 229/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0208 - val_loss: 0.0162 - val_mae: 0.0266\n",
            "Epoch 230/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0214 - val_loss: 0.0162 - val_mae: 0.0266\n",
            "Epoch 231/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0208 - val_loss: 0.0162 - val_mae: 0.0265\n",
            "Epoch 232/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0211 - val_loss: 0.0164 - val_mae: 0.0284\n",
            "Epoch 233/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0212 - val_loss: 0.0163 - val_mae: 0.0277\n",
            "Epoch 234/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0210 - val_loss: 0.0163 - val_mae: 0.0275\n",
            "Epoch 235/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0204 - val_loss: 0.0162 - val_mae: 0.0270\n",
            "Epoch 236/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0211 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 237/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0205 - val_loss: 0.0163 - val_mae: 0.0273\n",
            "Epoch 238/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0201 - val_loss: 0.0165 - val_mae: 0.0264\n",
            "Epoch 239/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0200 - val_loss: 0.0169 - val_mae: 0.0315\n",
            "Epoch 240/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0209 - val_loss: 0.0162 - val_mae: 0.0260\n",
            "Epoch 241/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0203 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 242/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0204 - val_loss: 0.0162 - val_mae: 0.0261\n",
            "Epoch 243/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0205 - val_loss: 0.0162 - val_mae: 0.0260\n",
            "Epoch 244/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0203 - val_loss: 0.0162 - val_mae: 0.0259\n",
            "Epoch 245/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0204 - val_loss: 0.0162 - val_mae: 0.0269\n",
            "Epoch 246/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0208 - val_loss: 0.0167 - val_mae: 0.0263\n",
            "Epoch 247/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0201 - val_loss: 0.0162 - val_mae: 0.0269\n",
            "Epoch 248/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0205 - val_loss: 0.0164 - val_mae: 0.0255\n",
            "Epoch 249/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0203 - val_loss: 0.0162 - val_mae: 0.0259\n",
            "Epoch 250/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0188 - val_loss: 0.0164 - val_mae: 0.0253\n",
            "Epoch 251/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0194 - val_loss: 0.0162 - val_mae: 0.0265\n",
            "Epoch 252/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0198 - val_loss: 0.0163 - val_mae: 0.0273\n",
            "Epoch 253/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0198 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 254/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0196 - val_loss: 0.0162 - val_mae: 0.0259\n",
            "Epoch 255/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0193 - val_loss: 0.0163 - val_mae: 0.0254\n",
            "Epoch 256/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0162 - val_mae: 0.0257\n",
            "Epoch 257/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0200 - val_loss: 0.0162 - val_mae: 0.0255\n",
            "Epoch 258/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0197 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 259/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0198 - val_loss: 0.0164 - val_mae: 0.0255\n",
            "Epoch 260/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0201 - val_loss: 0.0164 - val_mae: 0.0258\n",
            "Epoch 261/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0186 - val_loss: 0.0177 - val_mae: 0.0351\n",
            "Epoch 262/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0202 - val_loss: 0.0162 - val_mae: 0.0257\n",
            "Epoch 263/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0162 - val_mae: 0.0265\n",
            "Epoch 264/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0200 - val_loss: 0.0162 - val_mae: 0.0254\n",
            "Epoch 265/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0251\n",
            "Epoch 266/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0257\n",
            "Epoch 267/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0191 - val_loss: 0.0165 - val_mae: 0.0285\n",
            "Epoch 268/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0196 - val_loss: 0.0164 - val_mae: 0.0277\n",
            "Epoch 269/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0198 - val_loss: 0.0162 - val_mae: 0.0261\n",
            "Epoch 270/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0194 - val_loss: 0.0164 - val_mae: 0.0277\n",
            "Epoch 271/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0193 - val_loss: 0.0168 - val_mae: 0.0305\n",
            "Epoch 272/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0196 - val_loss: 0.0163 - val_mae: 0.0270\n",
            "Epoch 273/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0198 - val_loss: 0.0162 - val_mae: 0.0257\n",
            "Epoch 274/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0195 - val_loss: 0.0164 - val_mae: 0.0274\n",
            "Epoch 275/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0191 - val_loss: 0.0163 - val_mae: 0.0253\n",
            "Epoch 276/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0266\n",
            "Epoch 277/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0199 - val_loss: 0.0162 - val_mae: 0.0263\n",
            "Epoch 278/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0196 - val_loss: 0.0165 - val_mae: 0.0256\n",
            "Epoch 279/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0254\n",
            "Epoch 280/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0195 - val_loss: 0.0161 - val_mae: 0.0252\n",
            "Epoch 281/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0184 - val_loss: 0.0163 - val_mae: 0.0246\n",
            "Epoch 282/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0161 - val_mae: 0.0248\n",
            "Epoch 283/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0162 - val_mae: 0.0253\n",
            "Epoch 284/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0194 - val_loss: 0.0161 - val_mae: 0.0249\n",
            "Epoch 285/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0190 - val_loss: 0.0162 - val_mae: 0.0260\n",
            "Epoch 286/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0197 - val_loss: 0.0161 - val_mae: 0.0255\n",
            "Epoch 287/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0186 - val_loss: 0.0166 - val_mae: 0.0289\n",
            "Epoch 288/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0197 - val_loss: 0.0163 - val_mae: 0.0265\n",
            "Epoch 289/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0196 - val_loss: 0.0161 - val_mae: 0.0256\n",
            "Epoch 290/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0195 - val_loss: 0.0161 - val_mae: 0.0254\n",
            "Epoch 291/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0195 - val_loss: 0.0161 - val_mae: 0.0251\n",
            "Epoch 292/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0190 - val_loss: 0.0161 - val_mae: 0.0253\n",
            "Epoch 293/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0187 - val_loss: 0.0162 - val_mae: 0.0260\n",
            "Epoch 294/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0198 - val_loss: 0.0162 - val_mae: 0.0261\n",
            "Epoch 295/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0185 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 296/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0189 - val_loss: 0.0162 - val_mae: 0.0261\n",
            "Epoch 297/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0190 - val_loss: 0.0162 - val_mae: 0.0259\n",
            "Epoch 298/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0195 - val_loss: 0.0162 - val_mae: 0.0263\n",
            "Epoch 299/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0197 - val_loss: 0.0161 - val_mae: 0.0249\n",
            "Epoch 300/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0175 - val_loss: 0.0166 - val_mae: 0.0281\n",
            "Epoch 301/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0184 - val_loss: 0.0165 - val_mae: 0.0252\n",
            "Epoch 302/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0188 - val_loss: 0.0161 - val_mae: 0.0248\n",
            "Epoch 303/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0186 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 304/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0184 - val_loss: 0.0162 - val_mae: 0.0253\n",
            "Epoch 305/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0187 - val_loss: 0.0161 - val_mae: 0.0246\n",
            "Epoch 306/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0176 - val_loss: 0.0162 - val_mae: 0.0262\n",
            "Epoch 307/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0260\n",
            "Epoch 308/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0185 - val_loss: 0.0165 - val_mae: 0.0277\n",
            "Epoch 309/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0192 - val_loss: 0.0161 - val_mae: 0.0252\n",
            "Epoch 310/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0070 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 311/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0177 - val_loss: 0.0166 - val_mae: 0.0283\n",
            "Epoch 312/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0198 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 313/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0185 - val_loss: 0.0164 - val_mae: 0.0270\n",
            "Epoch 314/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0244\n",
            "Epoch 315/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0180 - val_loss: 0.0162 - val_mae: 0.0258\n",
            "Epoch 316/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0244\n",
            "Epoch 317/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0249\n",
            "Epoch 318/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0181 - val_loss: 0.0163 - val_mae: 0.0262\n",
            "Epoch 319/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0190 - val_loss: 0.0161 - val_mae: 0.0249\n",
            "Epoch 320/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0189 - val_loss: 0.0161 - val_mae: 0.0245\n",
            "Epoch 321/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0186 - val_loss: 0.0161 - val_mae: 0.0250\n",
            "Epoch 322/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0246\n",
            "Epoch 323/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0179 - val_loss: 0.0164 - val_mae: 0.0269\n",
            "Epoch 324/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0188 - val_loss: 0.0161 - val_mae: 0.0246\n",
            "Epoch 325/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0182 - val_loss: 0.0163 - val_mae: 0.0265\n",
            "Epoch 326/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0250\n",
            "Epoch 327/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0185 - val_loss: 0.0164 - val_mae: 0.0250\n",
            "Epoch 328/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0186 - val_loss: 0.0164 - val_mae: 0.0250\n",
            "Epoch 329/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0161 - val_mae: 0.0244\n",
            "Epoch 330/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0178 - val_loss: 0.0161 - val_mae: 0.0252\n",
            "Epoch 331/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0251\n",
            "Epoch 332/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0179 - val_loss: 0.0161 - val_mae: 0.0241\n",
            "Epoch 333/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0242\n",
            "Epoch 334/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0188 - val_loss: 0.0161 - val_mae: 0.0244\n",
            "Epoch 335/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0182 - val_loss: 0.0162 - val_mae: 0.0256\n",
            "Epoch 336/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0250\n",
            "Epoch 337/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0176 - val_loss: 0.0162 - val_mae: 0.0254\n",
            "Epoch 338/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0243\n",
            "Epoch 339/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0240\n",
            "Epoch 340/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0182 - val_loss: 0.0162 - val_mae: 0.0251\n",
            "Epoch 341/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0178 - val_loss: 0.0161 - val_mae: 0.0246\n",
            "Epoch 342/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0242\n",
            "Epoch 343/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 344/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0182 - val_loss: 0.0166 - val_mae: 0.0279\n",
            "Epoch 345/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0189 - val_loss: 0.0162 - val_mae: 0.0252\n",
            "Epoch 346/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0187 - val_loss: 0.0163 - val_mae: 0.0257\n",
            "Epoch 347/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0185 - val_loss: 0.0163 - val_mae: 0.0261\n",
            "Epoch 348/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 349/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0193 - val_loss: 0.0162 - val_mae: 0.0253\n",
            "Epoch 350/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0242\n",
            "Epoch 351/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0189 - val_loss: 0.0162 - val_mae: 0.0255\n",
            "Epoch 352/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0179 - val_loss: 0.0166 - val_mae: 0.0277\n",
            "Epoch 353/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0180 - val_loss: 0.0160 - val_mae: 0.0241\n",
            "Epoch 354/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0176 - val_loss: 0.0164 - val_mae: 0.0264\n",
            "Epoch 355/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0243\n",
            "Epoch 356/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0181 - val_loss: 0.0162 - val_mae: 0.0253\n",
            "Epoch 357/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0177 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 358/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0176 - val_loss: 0.0160 - val_mae: 0.0243\n",
            "Epoch 359/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0178 - val_loss: 0.0161 - val_mae: 0.0244\n",
            "Epoch 360/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0184 - val_loss: 0.0161 - val_mae: 0.0241\n",
            "Epoch 361/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0182 - val_loss: 0.0160 - val_mae: 0.0242\n",
            "Epoch 362/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0177 - val_loss: 0.0164 - val_mae: 0.0262\n",
            "Epoch 363/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0190 - val_loss: 0.0160 - val_mae: 0.0239\n",
            "Epoch 364/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0177 - val_loss: 0.0162 - val_mae: 0.0248\n",
            "Epoch 365/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0180 - val_loss: 0.0162 - val_mae: 0.0252\n",
            "Epoch 366/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0177 - val_loss: 0.0162 - val_mae: 0.0250\n",
            "Epoch 367/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0070 - mae: 0.0181 - val_loss: 0.0162 - val_mae: 0.0247\n",
            "Epoch 368/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0173 - val_loss: 0.0164 - val_mae: 0.0259\n",
            "Epoch 369/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0174 - val_loss: 0.0160 - val_mae: 0.0242\n",
            "Epoch 370/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0179 - val_loss: 0.0161 - val_mae: 0.0243\n",
            "Epoch 371/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0181 - val_loss: 0.0165 - val_mae: 0.0267\n",
            "Epoch 372/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0180 - val_loss: 0.0162 - val_mae: 0.0251\n",
            "Epoch 373/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0178 - val_loss: 0.0161 - val_mae: 0.0247\n",
            "Epoch 374/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0180 - val_loss: 0.0166 - val_mae: 0.0277\n",
            "Epoch 375/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0186 - val_loss: 0.0161 - val_mae: 0.0241\n",
            "Epoch 376/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0180 - val_loss: 0.0162 - val_mae: 0.0249\n",
            "Epoch 377/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0175 - val_loss: 0.0161 - val_mae: 0.0243\n",
            "Epoch 378/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0174 - val_loss: 0.0164 - val_mae: 0.0260\n",
            "Epoch 379/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0179 - val_loss: 0.0163 - val_mae: 0.0256\n",
            "Epoch 380/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0182 - val_loss: 0.0161 - val_mae: 0.0242\n",
            "Epoch 381/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0182 - val_loss: 0.0162 - val_mae: 0.0250\n",
            "Epoch 382/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0185 - val_loss: 0.0161 - val_mae: 0.0245\n",
            "Epoch 383/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0179 - val_loss: 0.0161 - val_mae: 0.0243\n",
            "Epoch 384/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0178 - val_loss: 0.0162 - val_mae: 0.0243\n",
            "Epoch 385/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0173 - val_loss: 0.0161 - val_mae: 0.0237\n",
            "Epoch 386/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0172 - val_loss: 0.0161 - val_mae: 0.0235\n",
            "Epoch 387/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0167 - val_loss: 0.0165 - val_mae: 0.0264\n",
            "Epoch 388/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0175 - val_loss: 0.0162 - val_mae: 0.0244\n",
            "Epoch 389/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0175 - val_loss: 0.0164 - val_mae: 0.0258\n",
            "Epoch 390/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0174 - val_loss: 0.0161 - val_mae: 0.0238\n",
            "Epoch 391/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0175 - val_loss: 0.0162 - val_mae: 0.0246\n",
            "Epoch 392/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0178 - val_loss: 0.0160 - val_mae: 0.0236\n",
            "Epoch 393/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0173 - val_loss: 0.0162 - val_mae: 0.0248\n",
            "Epoch 394/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0175 - val_loss: 0.0161 - val_mae: 0.0241\n",
            "Epoch 395/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0177 - val_loss: 0.0161 - val_mae: 0.0239\n",
            "Epoch 396/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0172 - val_loss: 0.0163 - val_mae: 0.0251\n",
            "Epoch 397/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0178 - val_loss: 0.0161 - val_mae: 0.0239\n",
            "Epoch 398/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0173 - val_loss: 0.0161 - val_mae: 0.0239\n",
            "Epoch 399/400\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0164 - val_loss: 0.0162 - val_mae: 0.0232\n",
            "Epoch 400/400\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0168 - val_loss: 0.0161 - val_mae: 0.0234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Y0CCWJz2EK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf81332b-fa51-4996-c718-cf60088226b5"
      },
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "# print(\"predictions =\\n\", np.round(predictions, decimals=3)[50])\n",
        "print(\"predictions =\\n\", predictions[50])\n",
        "print(\"actual =\\n\", outputs_test[50])\n",
        "\n",
        "correct_count = 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  label = np.argmax(predictions[i])\n",
        "  if outputs_test[i][label] == 1:\n",
        "    correct_count += 1\n",
        "\n",
        "print(correct_count)\n",
        "print(correct_count/len(outputs_test))\n",
        "\n",
        "\n",
        "# # Plot the predictions along with to the test data\n",
        "# plt.clf()\n",
        "# plt.title('Training data predicted vs actual values')\n",
        "# plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
        "# plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions =\n",
            " [9.9999261e-01 7.3389624e-06 6.0341992e-16]\n",
            "actual =\n",
            " [1. 0. 0.]\n",
            "121\n",
            "0.952755905511811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xn1-Rn9Cp_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7036a36-e375-45c8-951c-2cd4ef9f5792"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "  \n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpycqftii1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is 2204 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File \n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6aaca8-27fb-4e5e-81a9-a8ac0fe351fc"
      },
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file, model.h, is 13,626 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Realtime Classification of Sensor Data on Arduino\n",
        "\n",
        "Now it's time to switch back to the tutorial instructions and run our new model on the [Arduino Nano 33 BLE Sense](https://www.arduino.cc/en/Guide/NANO33BLE)"
      ]
    }
  ]
}